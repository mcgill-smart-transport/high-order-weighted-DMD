{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Example of using HW-DMD for traffic speed forecasting\n",
    "This notebook shows a minimal, reproducible example of using [HW-DMD](https://github.com/mcgill-smart-transport/high-order-weighted-DMD) [1] for short-term traffic speed forecasting on the [Seattle Inductive Loop Detector Dataset](https://github.com/zhiyongc/Seattle-Loop-Data). The original dataset contains every five-minute traffic speed recorded by 323 loop detectors in Seattle for the year 2015. Here we use the first month's data for demonstration. The locations of the detectors are shown in the following figure.\n",
    "\n",
    "<img src=\"DataLoop.png\" title=\"DataLoop\" width=\"500\"/>\n",
    "\n",
    "We use the traffic speed of the last ten intervals $[\\mathbf{v}_{t-1}, \\mathbf{v}_{t-2}, \\cdots, \\mathbf{v}_{t-10}]$ to forecast the traffic speed $\\mathbf{v}_{t}$ in the next time interval. This setting is the same as the [original paper](https://github.com/zhiyongc/Graph_Convolutional_LSTM) [2] that used this Seattle traffic dataset. We will see HW-DMD is comparable with many deep-learning models in the [original paper](https://github.com/zhiyongc/Graph_Convolutional_LSTM).\n",
    "\n",
    "References:\n",
    "- [1] Cheng, Z., Trepanier, M., & Sun, L. (2021). Real-time forecasting of metro origin-destination matrices with high-order weighted dynamic mode decomposition. arXiv preprint arXiv:2101.00466.\n",
    "- [2] Cui, Z., Henrickson, K., Ke, R., & Wang, Y. (2019). Traffic graph convolutional recurrent neural network: A deep learning framework for network-scale traffic learning and forecasting. IEEE Transactions on Intelligent Transportation Systems, 21(11), 4883-4894.\n",
    "\n",
    "## Define HW-DMD and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import svd, orth\n",
    "from numpy.linalg import pinv, eigh\n",
    "import time\n",
    "\n",
    "class HWDMD:\n",
    "    def __init__(self, h, rx, ry, rho=1, bs=1):\n",
    "        self.h = np.sort(h)\n",
    "        self.rx = rx\n",
    "        self.ry = ry\n",
    "        self.rho = rho # Forgetting ratio, i.e. weights\n",
    "        self.sigma = rho ** 0.5\n",
    "        self.bs = bs  # batch size\n",
    "        self.Ux = None\n",
    "        self.Uy = None\n",
    "        self.P = None\n",
    "        self.Qx = None\n",
    "        self.Qy = None\n",
    "        self.Qx_inv = None\n",
    "        self.n = None\n",
    "\n",
    "    def Uxi(self, i):\n",
    "        if i == len(self.h):  # The Ux for other features\n",
    "            return self.Ux[(i * self.n):, :]\n",
    "        else:  # The Ux for auto-regression\n",
    "            return self.Ux[(i * self.n):(i * self.n + self.n), :]\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.n, t = Y.shape\n",
    "        m = Y.shape[1]\n",
    "\n",
    "        num_bs = np.ceil(t / self.bs)  # Number of batches\n",
    "        weight = self.sigma ** np.repeat(np.arange(num_bs), self.bs)[:m][::-1]\n",
    "\n",
    "        X = X * weight\n",
    "        Y = Y * weight\n",
    "\n",
    "        [Ux, _, _] = svd(X, full_matrices=False)\n",
    "        [Uy, _, _] = svd(Y, full_matrices=False)\n",
    "        self.Ux = Ux[:, 0:self.rx]\n",
    "        self.Uy = Uy[:, 0:self.ry]\n",
    "\n",
    "        Xtilde = self.Ux.T @ X\n",
    "        Ytilde = self.Uy.T @ Y\n",
    "        self.P = Ytilde @ Xtilde.T\n",
    "        self.Qx = Xtilde @ Xtilde.T\n",
    "        self.Qy = Ytilde @ Ytilde.T\n",
    "        self.Qx_inv = pinv(self.Qx)\n",
    "\n",
    "    def forecast(self, X):\n",
    "        \"\"\"Return the one_step forecast for **staggered** data.\"\"\"\n",
    "        nn = len(self.h)\n",
    "        part2 = 0\n",
    "        for i in range(nn):\n",
    "            part2 += self.Uxi(i).T @ self.Uy @ (self.Uy.T @ X[i * self.n:(i * self.n + self.n), :])\n",
    "\n",
    "        # When there are external features\n",
    "        if X.shape[0] > nn*self.n:\n",
    "            part2 += self.Uxi(nn).T @ X[nn * self.n:, :]\n",
    "\n",
    "        return self.Uy @ self.P @ self.Qx_inv @ part2\n",
    "\n",
    "    def update_model(self, X, Y):\n",
    "        \"\"\"Update the model coefficients using the new data (staggered). Does not change the buffer data\"\"\"\n",
    "        if X.shape[1] != self.bs or Y.shape[1] != self.bs:\n",
    "            raise ValueError('Number of columns does not equal to batchsize.')\n",
    "        Ybar = self.Uy @ (self.Uy.T @ Y)\n",
    "        Xbar = self.Ux @ (self.Ux.T @ X)\n",
    "\n",
    "        Ux = orth(X - Xbar)\n",
    "        Uy = orth(Y - Ybar)\n",
    "        self.Ux = np.concatenate([self.Ux, Ux], axis=1)\n",
    "        self.Uy = np.concatenate([self.Uy, Uy], axis=1)\n",
    "        self.Qx = np.pad(self.Qx, ((0, Ux.shape[1]), (0, Ux.shape[1])), 'constant', constant_values=0)\n",
    "        self.Qy = np.pad(self.Qy, ((0, Uy.shape[1]), (0, Uy.shape[1])), 'constant', constant_values=0)\n",
    "        self.P = np.pad(self.P, ((0, Uy.shape[1]), (0, Ux.shape[1])), 'constant', constant_values=0)\n",
    "\n",
    "        Ytilde = self.Uy.T @ Y\n",
    "        Xtilde = self.Ux.T @ X\n",
    "        self.P = self.rho * self.P + Ytilde @ Xtilde.T\n",
    "        self.Qx = self.rho * self.Qx + Xtilde @ Xtilde.T\n",
    "        self.Qy = self.rho * self.Qy + Ytilde @ Ytilde.T\n",
    "\n",
    "        # Compress rx and ry\n",
    "        evalue_x, evector_x = eigh(self.Qx)\n",
    "        evalue_y, evector_y = eigh(self.Qy)\n",
    "        evector_x = evector_x[:, -self.rx:]\n",
    "        evector_y = evector_y[:, -self.ry:]\n",
    "        self.Ux = self.Ux @ evector_x\n",
    "        self.Uy = self.Uy @ evector_y\n",
    "        self.Qx = np.diag(evalue_x[-self.rx:])\n",
    "        self.Qy = np.diag(evalue_y[-self.ry:])\n",
    "        self.P = evector_y.T @ self.P @ evector_x\n",
    "\n",
    "        # Update\n",
    "        self.Qx_inv = pinv(self.Qx)\n",
    "\n",
    "\n",
    "# Some helper functions\n",
    "def stagger_data(data, h):\n",
    "    # Help to prepare input and target\n",
    "    h.sort()\n",
    "    len_h = len(h)\n",
    "    n, m = data.shape\n",
    "    max_h = max(h)\n",
    "\n",
    "    Y = data[:, max_h:]\n",
    "    X = np.zeros((n * len_h, m - max_h), dtype=data.dtype)\n",
    "    for i in range(len_h):\n",
    "        X[i * n: i * n + n, :] = data[:, max_h - h[i]:m - h[i]]\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def RMSE(f0, f1):\n",
    "    return np.sqrt(np.mean((f0 - f1) ** 2))\n",
    "\n",
    "\n",
    "def MAPE(f0, f1):\n",
    "    return np.mean(np.abs(f0-f1)/f0) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_pickle('..//data//Seattle_one_month_speed_2015.pickle')\n",
    "\n",
    "train_idx = np.arange(0, 25*24*12)  # Use the first 25 days for training\n",
    "test_idx = np.arange(25*24*12, 31*24*12)  # The next 6 days for testing\n",
    "h = np.arange(1,11)\n",
    "X, Y = stagger_data(data.iloc[train_idx, :].values.T, h)\n",
    "X_test, Y_test = stagger_data(data.iloc[test_idx[0]-len(h):, :].values.T, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## V1: a constant HW-DMD without online update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 43.956s\n",
      "RMSE:4.579\n",
      "MAPE:8.172%\n"
     ]
    }
   ],
   "source": [
    "# The optimal `rx` is usually larger than `ry`\n",
    "model = HWDMD(h, rx=300, ry=90, rho=0.999, bs=12)\n",
    "t0 = time.time()\n",
    "model.fit(X, Y)\n",
    "Y_predict = model.forecast(X_test)\n",
    "print(f'Total time: {time.time()-t0:.3f}s')\n",
    "print(f'RMSE:{RMSE(Y_test, Y_predict):.3f}')\n",
    "print(f'MAPE:{MAPE(Y_test, Y_predict):.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## V2: online update HW-DMD every one hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 4.624s\n",
      "RMSE:4.504\n",
      "MAPE:7.955%\n"
     ]
    }
   ],
   "source": [
    "bs = 12  # The batch size in the online update\n",
    "Y_predict2 = []\n",
    "t0 = time.time()\n",
    "for i in range(144):\n",
    "    X_today = X_test[:, i*bs:(i+1)*bs]\n",
    "    Y_today = Y_test[:, i*bs:(i+1)*bs]\n",
    "    Y_predict2.append(model.forecast(X_today))\n",
    "    model.update_model(X_today, Y_today)\n",
    "print(f'Total time: {time.time()-t0:.3f}s')\n",
    "Y_predict2 = np.concatenate(Y_predict2, axis=1)\n",
    "print(f'RMSE:{RMSE(Y_test, Y_predict2):.3f}')\n",
    "print(f'MAPE:{MAPE(Y_test, Y_predict2):.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: RMSE=4.63, MAPE=6.01% for the best model in the [original paper](https://github.com/zhiyongc/Graph_Convolutional_LSTM) [2] that used this Seattle traffic dataset. They probably used the entirely year data? This notebook uses one-month data.\n",
    "\n",
    "## Summary\n",
    "- Implementing HW-DMD from raw is simple (around one hundred lines).\n",
    "- The estimation is fast.\n",
    "- The performance is comparable to some complex deep-learning models.\n",
    "- Properly use the online update can improve forecast accuracy. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6dad9d21e8993add192270771356756105870408952531894fdd3e5790b8a9a8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('macer': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
