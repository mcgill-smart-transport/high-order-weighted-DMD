{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNq8QlNjEE9R"
   },
   "source": [
    "# Import data and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3930,
     "status": "ok",
     "timestamp": 1631224980908,
     "user": {
      "displayName": "Zhanhong Cheng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghf9tjnpXtH8UqcRXabhSLJr8JznMk-NALJeT6E_g=s64",
      "userId": "09964432197138505126"
     },
     "user_tz": 240
    },
    "id": "hP1Vw220XMaL",
    "outputId": "6451b7c6-922e-4d55-f699-158647c8a0f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 2.x\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(1)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    print(\n",
    "      '\\n\\nThis error most likely means that this notebook is not '\n",
    "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
    "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
    "    raise SystemError('GPU device not found')\n",
    "\n",
    "def cpu():\n",
    "    with tf.device('/cpu:0'):\n",
    "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
    "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
    "    return tf.math.reduce_sum(net_cpu)\n",
    "\n",
    "def gpu():\n",
    "    with tf.device('/device:GPU:0'):\n",
    "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
    "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
    "    return tf.math.reduce_sum(net_gpu)\n",
    "\n",
    "def reset_random_seeds(n=1):\n",
    "    os.environ['PYTHONHASHSEED']=str(n)\n",
    "    tf.random.set_seed(n)\n",
    "    np.random.seed(n)\n",
    "    random.seed(n)\n",
    "\n",
    "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
    "cpu()\n",
    "gpu()\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ZXCbaZLXZrz"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import gc\n",
    "from sklearn.metrics import r2_score\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "def stagger_data(data, h):\n",
    "    \"\"\"\n",
    "    >>> i = np.array([[1, 2, 3, 4, 5, 6], [7, 8, 9, 10, 11, 12]])\n",
    "    >>> stagger_data(i, [1, 3])\n",
    "    (array([[ 3,  4,  5],\n",
    "           [ 9, 10, 11],\n",
    "           [ 1,  2,  3],\n",
    "           [ 7,  8,  9]]), array([[ 4,  5,  6],\n",
    "           [10, 11, 12]]))\n",
    "    \"\"\"\n",
    "    h.sort()\n",
    "    len_h = len(h)\n",
    "    n, m = data.shape\n",
    "    max_h = max(h)\n",
    "\n",
    "    Y = data[:, max_h:]\n",
    "    X = np.zeros((n * len_h, m - max_h), dtype=data.dtype)\n",
    "    for i in range(len_h):\n",
    "        X[i * n: i * n + n, :] = data[:, max_h - h[i]:m - h[i]]\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def remove_weekends(data, start=0, bs=36):\n",
    "    _, m = data.shape\n",
    "    n_day = int(m / bs)\n",
    "    weekday = np.concatenate([np.arange(start, 7) % 7, np.arange(n_day) % 7])[:n_day]\n",
    "    weekday = np.repeat(weekday, bs)\n",
    "    return data[:, weekday < 5]\n",
    "\n",
    "\n",
    "def get_flow1(od, s, dir='o', num_s=159):\n",
    "    \"\"\"Get the flow of station `s`\"\"\"\n",
    "    n = od.shape[0]\n",
    "    if dir == 'o':\n",
    "        idx = np.arange(s, n, num_s)\n",
    "    elif dir == 'd':\n",
    "        idx = np.arange((s * num_s), (s * num_s + num_s))\n",
    "    return np.sum(od[idx, :], axis=0)\n",
    "\n",
    "\n",
    "def od2flow(od, s_list=None, dir='o', num_s=159):\n",
    "    if s_list is None:\n",
    "        s_list = range(num_s)\n",
    "\n",
    "    n_s = len(s_list)\n",
    "    flow = np.zeros((n_s, od.shape[1]), dtype=np.float32)\n",
    "    for i, s in enumerate(s_list):\n",
    "        flow[i, :] = get_flow1(od, s, dir, num_s)\n",
    "    return flow\n",
    "\n",
    "\n",
    "def RMSE(f0, f1, axis=None):\n",
    "    return np.sqrt(np.mean((f0 - f1) ** 2, axis))\n",
    "\n",
    "\n",
    "def SMAPE(real, predict):\n",
    "    a = real.ravel().copy()\n",
    "    b = predict.ravel().copy()\n",
    "    mask = ((a>0) & (b>0))\n",
    "    a = a[mask]\n",
    "    b = b[mask]\n",
    "    return 2*np.mean(np.abs(a-b)/(np.abs(a)+np.abs(b)))\n",
    "\n",
    "\n",
    "def WMAPE(real, predict):\n",
    "    e = np.sum(np.abs(real - predict))/np.sum(np.abs(real))\n",
    "    return e\n",
    "\n",
    "\n",
    "def MAE(real, predict):\n",
    "    return np.mean(np.abs(real - predict))\n",
    "\n",
    "def MSE(f0, f1, axis=None):\n",
    "    return np.mean((f0 - f1) ** 2, axis)\n",
    "\n",
    "\n",
    "def get_score(real, predict, real_flow, predict_flow):\n",
    "    print('RMSE of OD: {}'.format(RMSE(real, predict)))\n",
    "    print('WMAPE of OD: {}'.format(WMAPE(real, predict)))\n",
    "    print('SMAPE of OD: {}'.format(SMAPE(real, predict)))\n",
    "    print('MAE of OD: {}'.format(MAE(real, predict)))\n",
    "    print('r2 of OD: {}'.format(r2_score(real.ravel(), predict.ravel())))\n",
    "    print('\\n')\n",
    "    print('RMSE of flow: {}'.format(RMSE(real_flow, predict_flow)))\n",
    "    print('WMAPE of flow: {}'.format(WMAPE(real_flow, predict_flow)))\n",
    "    print('SMAPE of flow: {}'.format(SMAPE(real_flow, predict_flow)))\n",
    "    print('MAE of flow: {}'.format(MAE(real_flow, predict_flow)))\n",
    "    print('r2 of flow: {}'.format(r2_score(real_flow.ravel(), predict_flow.ravel())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3KzTgYUw4Uez"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ts5me2dHS9JS"
   },
   "outputs": [],
   "source": [
    "data0 = loadmat('drive//MyDrive//data//Huangzhou_OD.mat')\n",
    "data0 = data0['OD']\n",
    "data0 = remove_weekends(data0, start=1)\n",
    "num_s = 80\n",
    "\n",
    "# Subtract the mean of the training set\n",
    "data = data0.astype(np.float64)\n",
    "data_mean = data[:, 0:14*36].reshape([num_s*num_s, 36, -1], order='F')\n",
    "data_mean = data_mean.mean(axis=2)\n",
    "for i in range(19):\n",
    "    data[:, i*36:(i+1)*36] = data[:, i*36:(i+1)*36] - data_mean\n",
    "\n",
    "# Prepare lagged flow as a feature\n",
    "flow0 = od2flow(data, num_s=num_s)\n",
    "flow = np.zeros((flow0.shape[0]*2, flow0.shape[1]), dtype=flow0.dtype)\n",
    "flow[0:flow0.shape[0], :] = flow0\n",
    "flow[flow0.shape[0]:, 1:] = flow0[:, 0:-1]\n",
    "\n",
    "train_idx = np.arange(0, 14*36)\n",
    "test_idx = np.arange(36*14, 36*19)\n",
    "\n",
    "h = [3, 4, 5, 6, 7, 8, 9, 10]\n",
    "# Prepare train and validataion data\n",
    "X_train, Y_train = stagger_data(data[:, train_idx], h)\n",
    "m_train = X_train.shape[1]\n",
    "X_train = np.concatenate([X_train, flow[:, train_idx][:, -m_train-1:-1]/num_s]).T\n",
    "Y_train = Y_train.T\n",
    "\n",
    "# Split training and validataion set\n",
    "reset_random_seeds(0)\n",
    "random_idx = np.random.permutation(m_train)\n",
    "train_idx = random_idx[0:int(np.floor(m_train*0.8))]\n",
    "validate_idx = random_idx[int(np.floor(m_train*0.8)):]\n",
    "x_train = X_train[train_idx, :]\n",
    "y_train = Y_train[train_idx, :]\n",
    "x_validate = X_train[validate_idx, :]\n",
    "y_validate = Y_train[validate_idx, :]\n",
    "\n",
    "\n",
    "X_test, Y_test = stagger_data(data[:, (test_idx[0]-max(h)):(test_idx[-1]+1)], h)\n",
    "X_test = np.concatenate([X_test, flow[:, test_idx-1]/num_s]).T\n",
    "Y_test = Y_test.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuWHNahF4b5f"
   },
   "source": [
    "# Select model order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9RnJ5Jq5lf2_"
   },
   "outputs": [],
   "source": [
    "def create_net(n_input=num_s*num_s*10, n_hidden=50, activation=None):\n",
    "    seq = keras.Sequential(\n",
    "      [\n",
    "      layers.Dense(n_hidden, input_shape=(n_input,), \n",
    "                   activation=activation,\n",
    "                  #  kernel_regularizer=tf.keras.regularizers.L2(0.0001)\n",
    "                   ),\n",
    "      layers.Dense(num_s*num_s,\n",
    "                  #  kernel_regularizer=tf.keras.regularizers.L2(0.0001)\n",
    "                   )\n",
    "      ]\n",
    "    )\n",
    "    return seq\n",
    "\n",
    "call_back = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=20, verbose=0, mode='auto',\n",
    "    baseline=None, restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 694814,
     "status": "ok",
     "timestamp": 1631225677080,
     "user": {
      "displayName": "Zhanhong Cheng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghf9tjnpXtH8UqcRXabhSLJr8JznMk-NALJeT6E_g=s64",
      "userId": "09964432197138505126"
     },
     "user_tz": 240
    },
    "id": "q-38tw9yYtVW",
    "outputId": "a5d05760-7ab9-4933-bc54-7bdf4fb2decf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation linear hidden_layers 10, val_loss 7.687071361927071, current best val_loss 7.687071361927071\n",
      "Activation linear hidden_layers 20, val_loss 7.7264406078993675, current best val_loss 7.687071361927071\n",
      "Activation linear hidden_layers 30, val_loss 7.834968065974688, current best val_loss 7.687071361927071\n",
      "Activation linear hidden_layers 40, val_loss 7.881966850974343, current best val_loss 7.687071361927071\n",
      "Activation linear hidden_layers 50, val_loss 7.904993095783272, current best val_loss 7.687071361927071\n",
      "Activation linear hidden_layers 60, val_loss 7.723924906566889, current best val_loss 7.687071361927071\n",
      "Activation linear hidden_layers 70, val_loss 7.914479140079383, current best val_loss 7.687071361927071\n",
      "Activation linear hidden_layers 80, val_loss 7.839710818396674, current best val_loss 7.687071361927071\n",
      "Activation linear hidden_layers 90, val_loss 7.918717567366783, current best val_loss 7.687071361927071\n",
      "Activation linear hidden_layers 100, val_loss 7.888565891920918, current best val_loss 7.687071361927071\n",
      "Activation sigmoid hidden_layers 10, val_loss 12.391006286698158, current best val_loss 7.687071361927071\n",
      "Activation sigmoid hidden_layers 20, val_loss 11.984400537278917, current best val_loss 7.687071361927071\n",
      "Activation sigmoid hidden_layers 30, val_loss 11.746901107556893, current best val_loss 7.687071361927071\n",
      "Activation sigmoid hidden_layers 40, val_loss 11.39126583060833, current best val_loss 7.687071361927071\n",
      "Activation sigmoid hidden_layers 50, val_loss 11.149263564986413, current best val_loss 7.687071361927071\n",
      "Activation sigmoid hidden_layers 60, val_loss 11.027932436779292, current best val_loss 7.687071361927071\n",
      "Activation sigmoid hidden_layers 70, val_loss 11.03592001789748, current best val_loss 7.687071361927071\n",
      "Activation sigmoid hidden_layers 80, val_loss 10.848881798561173, current best val_loss 7.687071361927071\n",
      "Activation sigmoid hidden_layers 90, val_loss 10.948321400266705, current best val_loss 7.687071361927071\n",
      "Activation sigmoid hidden_layers 100, val_loss 10.882852149732186, current best val_loss 7.687071361927071\n",
      "Activation relu hidden_layers 10, val_loss 8.44204656042234, current best val_loss 7.687071361927071\n",
      "Activation relu hidden_layers 20, val_loss 8.589135911729601, current best val_loss 7.687071361927071\n",
      "Activation relu hidden_layers 30, val_loss 8.830984009636772, current best val_loss 7.687071361927071\n",
      "Activation relu hidden_layers 40, val_loss 9.292371817309448, current best val_loss 7.687071361927071\n",
      "Activation relu hidden_layers 50, val_loss 10.045209133263791, current best val_loss 7.687071361927071\n",
      "Activation relu hidden_layers 60, val_loss 10.844609751845852, current best val_loss 7.687071361927071\n",
      "Activation relu hidden_layers 70, val_loss 10.807251930236816, current best val_loss 7.687071361927071\n",
      "Activation relu hidden_layers 80, val_loss 11.784474700388282, current best val_loss 7.687071361927071\n",
      "Activation relu hidden_layers 90, val_loss 13.146795841178509, current best val_loss 7.687071361927071\n",
      "Activation relu hidden_layers 100, val_loss 13.828900568413012, current best val_loss 7.687071361927071\n"
     ]
    }
   ],
   "source": [
    "activation_list = ['linear', 'sigmoid', 'relu']\n",
    "n_hidden_list = np.linspace(10, 100, 10, dtype=np.int)\n",
    "best_e = 10000\n",
    "\n",
    "for activation in activation_list:\n",
    "    for n_hidden in n_hidden_list:\n",
    "        tf.keras.backend.clear_session()\n",
    "        reset_random_seeds(0)\n",
    "        seq = create_net(n_input=x_train.shape[1], n_hidden=n_hidden, activation=activation)\n",
    "        gc.collect()\n",
    "        seq.compile(loss=\"mean_squared_error\", optimizer=\"RMSprop\")\n",
    "        seq.fit(\n",
    "            x=x_train,\n",
    "            y=y_train,\n",
    "            batch_size=32,\n",
    "            epochs=200,\n",
    "            verbose=0,\n",
    "            shuffle=True,\n",
    "            validation_data =(x_validate, y_validate),\n",
    "            callbacks=[call_back],\n",
    "        )\n",
    "        val_loss = min(seq.history.history['val_loss'])\n",
    "\n",
    "        if val_loss < best_e:\n",
    "            best_e = val_loss\n",
    "            best_model = seq\n",
    "        print('Activation {} hidden_layers {}, val_loss {}, current best val_loss {}'.format(activation, n_hidden, val_loss, best_e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8MsxO1f-sO4"
   },
   "source": [
    "# Multistep forecast for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5586,
     "status": "ok",
     "timestamp": 1631225682652,
     "user": {
      "displayName": "Zhanhong Cheng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghf9tjnpXtH8UqcRXabhSLJr8JznMk-NALJeT6E_g=s64",
      "userId": "09964432197138505126"
     },
     "user_tz": 240
    },
    "id": "VlwFgpsyqVrC",
    "outputId": "9cdd8705-6a94-40e3-c57c-114e8acda38b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "878"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-step forecast\n",
    "seq = best_model\n",
    "X1 = np.concatenate([X_train, X_test], axis=0)\n",
    "predict_OD1 = seq.predict(X1)\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# Two-step forecast\n",
    "n = data.shape[0]\n",
    "nh = len(h)\n",
    "X2 = np.concatenate([X_train, X_test])\n",
    "# Reuse one-step forecast OD\n",
    "X2[3:, 0:n] = predict_OD1[0:-3, :]\n",
    "# Reuse one-step forecast flow\n",
    "X2[3:, n*nh:n*nh+num_s] = od2flow(predict_OD1[2:-1, :].T, num_s=num_s).T/num_s\n",
    "predict_OD2 = seq.predict(X2)\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# Three-step forecast\n",
    "X3 = np.concatenate([X_train, X_test])\n",
    "# Reuse one and two-step forecast OD\n",
    "X3[4:, 0:n] = predict_OD2[1:-3, :]\n",
    "X3[4:, n:2*n] = predict_OD1[0:-4, :]\n",
    "# Reuse one and two-step forecast flow\n",
    "X3[4:, n*nh:n*nh+num_s] = od2flow(predict_OD2[3:-1, :].T, num_s=num_s).T/num_s\n",
    "X3[4:, n*nh+num_s:] = od2flow(predict_OD1[2:-2, :].T, num_s=num_s).T/num_s\n",
    "predict_OD3 = seq.predict(X3)\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yV2OOG8PlVz"
   },
   "source": [
    "# Add mean back and save results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1618,
     "status": "ok",
     "timestamp": 1631225684263,
     "user": {
      "displayName": "Zhanhong Cheng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghf9tjnpXtH8UqcRXabhSLJr8JznMk-NALJeT6E_g=s64",
      "userId": "09964432197138505126"
     },
     "user_tz": 240
    },
    "id": "c677160QNGVr",
    "outputId": "95054ca8-1eb1-40dd-9eed-eab550f8a2c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The result of 1-step prediction: \n",
      "\n",
      "RMSE of OD: 3.9719648466476154\n",
      "WMAPE of OD: 0.3357734532637882\n",
      "SMAPE of OD: 0.4712526885640867\n",
      "MAE of OD: 1.8040434348136745\n",
      "r2 of OD: 0.9078092935222805\n",
      "\n",
      "\n",
      "RMSE of flow: 67.16141510009766\n",
      "WMAPE of flow: 0.09000366181135178\n",
      "SMAPE of flow: 0.13968013226985931\n",
      "MAE of flow: 38.68572998046875\n",
      "r2 of flow: 0.9809799633289706\n",
      "\n",
      " The result of 2-step prediction: \n",
      "\n",
      "RMSE of OD: 4.012984099561706\n",
      "WMAPE of OD: 0.33630288879958387\n",
      "SMAPE of OD: 0.47392898437843534\n",
      "MAE of OD: 1.8068879857846496\n",
      "r2 of OD: 0.9058953186107709\n",
      "\n",
      "\n",
      "RMSE of flow: 68.82543182373047\n",
      "WMAPE of flow: 0.0922432690858841\n",
      "SMAPE of flow: 0.14330947399139404\n",
      "MAE of flow: 39.64836883544922\n",
      "r2 of flow: 0.9800257905127567\n",
      "\n",
      " The result of 3-step prediction: \n",
      "\n",
      "RMSE of OD: 4.047180802658473\n",
      "WMAPE of OD: 0.33652981381422836\n",
      "SMAPE of OD: 0.4770053964984703\n",
      "MAE of OD: 1.8081072083851415\n",
      "r2 of OD: 0.9042846562288088\n",
      "\n",
      "\n",
      "RMSE of flow: 70.76851654052734\n",
      "WMAPE of flow: 0.09501465409994125\n",
      "SMAPE of flow: 0.1471409797668457\n",
      "MAE of flow: 40.83957290649414\n",
      "r2 of flow: 0.9788820430276758\n"
     ]
    }
   ],
   "source": [
    "real_OD = data0[:, test_idx]\n",
    "real_flow = od2flow(real_OD, num_s=num_s)\n",
    "\n",
    "# Add mean values\n",
    "predict_OD1=predict_OD1[-180:, :].T\n",
    "for i in range(predict_OD1.shape[1]):\n",
    "    predict_OD1[:,i] += data_mean[:, i%36]\n",
    "predict_flow1 = od2flow(predict_OD1, num_s=num_s)\n",
    "print(\"\\n The result of 1-step prediction: \\n\")\n",
    "get_score(real_OD, predict_OD1, real_flow, predict_flow1)\n",
    "\n",
    "predict_OD2=predict_OD2[-180:, :].T\n",
    "for i in range(predict_OD2.shape[1]):\n",
    "    predict_OD2[:,i] += data_mean[:, i%36]\n",
    "predict_flow2 = od2flow(predict_OD2, num_s=num_s)\n",
    "print(\"\\n The result of 2-step prediction: \\n\")\n",
    "get_score(real_OD, predict_OD2, real_flow, predict_flow2)\n",
    "\n",
    "\n",
    "predict_OD3=predict_OD3[-180:, :].T\n",
    "for i in range(predict_OD3.shape[1]):\n",
    "    predict_OD3[:,i] += data_mean[:, i%36]\n",
    "predict_flow3 = od2flow(predict_OD3, num_s=num_s)\n",
    "print(\"\\n The result of 3-step prediction: \\n\")\n",
    "get_score(real_OD, predict_OD3, real_flow, predict_flow3)\n",
    "\n",
    "np.savez_compressed('/content/drive/MyDrive/data/Hangzhou_OD_FNN_step1.npz', data=predict_OD1)\n",
    "np.savez_compressed('/content/drive/MyDrive/data/Hangzhou_OD_FNN_step2.npz', data=predict_OD2)\n",
    "np.savez_compressed('/content/drive/MyDrive/data/Hangzhou_OD_FNN_step3.npz', data=predict_OD3)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hangzhou_FNN_sub_mean_2boarding.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
