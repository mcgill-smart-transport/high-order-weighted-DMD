{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Temporal regularized matrix factorization (TRMF) for metro OD forecasting. Code is adapted from [https://github.com/xinychen/transdim](https://github.com/xinychen/transdim).\n",
    "\n",
    "Original paper for TRMF:\n",
    "- Hsiang-Fu Yu, Nikhil Rao, Inderjit S. Dhillon, 2016. Temporal regularized matrix factorization for high-dimensional time series prediction. 30th Conference on Neural Information Processing Systems (NIPS 2016).\n",
    "\n",
    "# Define functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functions import *\n",
    "from numpy.linalg import inv as inv\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "def reset_random_seeds(n=1):\n",
    "    os.environ['PYTHONHASHSEED'] = str(n)\n",
    "    np.random.seed(n)\n",
    "    random.seed(n)\n",
    "\n",
    "\n",
    "def kr_prod(a, b):\n",
    "    return np.einsum('ir, jr -> ijr', a, b).reshape(a.shape[0] * b.shape[0], -1)\n",
    "\n",
    "\n",
    "def TRMF(train_data, init, time_lags, lambda_w, lambda_x, lambda_theta, eta, maxiter, multi_steps=1, display=10):\n",
    "    start = time.time()\n",
    "    W = init[\"W\"]\n",
    "    X = init[\"X\"]\n",
    "    theta = init[\"theta\"]\n",
    "\n",
    "    dim1, dim2 = train_data.shape\n",
    "    binary_mat = np.zeros((dim1, dim2))\n",
    "    position = np.where((train_data != 0))\n",
    "    binary_mat[position] = 1\n",
    "\n",
    "    d = len(time_lags)\n",
    "    r = theta.shape[1]\n",
    "\n",
    "    for iter in range(maxiter):\n",
    "        if (iter + 1) % display == 0:\n",
    "            print('Time step: {} time {}'.format(iter + 1, time.time() - start))\n",
    "        var1 = X.T\n",
    "        var2 = kr_prod(var1, var1)\n",
    "        var3 = np.matmul(var2, binary_mat.T)\n",
    "        var4 = np.matmul(var1, train_data.T)\n",
    "        for i in range(dim1):\n",
    "            W[i, :] = np.matmul(inv((var3[:, i].reshape([r, r])) + lambda_w * np.eye(r)), var4[:, i])\n",
    "\n",
    "        var1 = W.T\n",
    "        var2 = kr_prod(var1, var1)\n",
    "        var3 = np.matmul(var2, binary_mat)\n",
    "        var4 = np.matmul(var1, train_data)\n",
    "        for t in range(dim2):\n",
    "            Mt = np.zeros((r, r))\n",
    "            Nt = np.zeros(r)\n",
    "            if t < max(time_lags):\n",
    "                Pt = np.zeros((r, r))\n",
    "                Qt = np.zeros(r)\n",
    "            else:\n",
    "                Pt = np.eye(r)\n",
    "                Qt = np.einsum('ij, ij -> j', theta, X[t - time_lags, :])\n",
    "            if t < dim2 - np.min(time_lags):\n",
    "                if t >= np.max(time_lags) and t < dim2 - np.max(time_lags):\n",
    "                    index = list(range(0, d))\n",
    "                else:\n",
    "                    index = list(np.where((t + time_lags >= np.max(time_lags)) & (t + time_lags < dim2)))[0]\n",
    "                for k in index:\n",
    "                    theta0 = theta.copy()\n",
    "                    theta0[k, :] = 0\n",
    "                    Mt = Mt + np.diag(theta[k, :] ** 2)\n",
    "                    Nt = Nt + np.multiply(theta[k, :], (X[t + time_lags[k], :]\n",
    "                                                        - np.einsum('ij, ij -> j', theta0,\n",
    "                                                                    X[t + time_lags[k] - time_lags, :])))\n",
    "                X[t, :] = np.matmul(inv(var3[:, t].reshape([r, r])\n",
    "                                        + lambda_x * Pt + lambda_x * Mt + lambda_x * eta * np.eye(r)),\n",
    "                                    (var4[:, t] + lambda_x * Qt + lambda_x * Nt))\n",
    "            elif t >= dim2 - np.min(time_lags):\n",
    "                X[t, :] = np.matmul(inv(var3[:, t].reshape([r, r]) + lambda_x * Pt\n",
    "                                        + lambda_x * eta * np.eye(r)), (var4[:, t] + Qt))\n",
    "        for k in range(d):\n",
    "            var1 = X[np.max(time_lags) - time_lags[k]: dim2 - time_lags[k], :]\n",
    "            var2 = inv(np.diag(np.einsum('ij, ij -> j', var1, var1)) + (lambda_theta / lambda_x) * np.eye(r))\n",
    "            var3 = np.zeros(r)\n",
    "            for t in range(np.max(time_lags) - time_lags[k], dim2 - time_lags[k]):\n",
    "                var3 = var3 + np.multiply(X[t, :],\n",
    "                                          (X[t + time_lags[k], :]\n",
    "                                           - np.einsum('ij, ij -> j', theta, X[t + time_lags[k] - time_lags, :])\n",
    "                                           + np.multiply(theta[k, :], X[t, :])))\n",
    "            theta[k, :] = np.matmul(var2, var3)\n",
    "\n",
    "    X_new = np.zeros((dim2 + multi_steps, rank))\n",
    "    X_new[0: dim2, :] = X.copy()\n",
    "    for step in range(multi_steps):\n",
    "        X_new[dim2 + step, :] = np.einsum('ij, ij -> j', theta, X_new[dim2 + step - time_lags, :])\n",
    "\n",
    "    return W, X_new, theta, np.matmul(W, X_new[dim2: dim2 + multi_steps, :].T)\n",
    "\n",
    "\n",
    "def OnlineTRMF(sparse_vec, init, lambda_x, time_lags):\n",
    "    W = init[\"W\"]\n",
    "    X = init[\"X\"]\n",
    "    theta = init[\"theta\"]\n",
    "    dim = sparse_vec.shape[0]\n",
    "    t, rank = X.shape\n",
    "    position = np.where(sparse_vec != 0)\n",
    "    binary_vec = np.zeros(dim)\n",
    "    binary_vec[position] = 1\n",
    "\n",
    "    xt_tilde = np.einsum('ij, ij -> j', theta, X[t - 1 - time_lags, :])\n",
    "    var1 = W.T\n",
    "    var2 = kr_prod(var1, var1)\n",
    "    var_mu = np.matmul(var1, sparse_vec) + lambda_x * xt_tilde\n",
    "    inv_var_Lambda = inv(np.matmul(var2, binary_vec).reshape([rank, rank]) + lambda_x * np.eye(rank))\n",
    "    X[t - 1, :] = np.matmul(inv_var_Lambda, var_mu)\n",
    "    return X\n",
    "\n",
    "\n",
    "def st_prediction(train_data, time_lags, lambda_w, lambda_x, lambda_theta, eta,\n",
    "                  rank, pred_time_steps, maxiter, multi_steps=1, display=100):\n",
    "    start = time.time()\n",
    "    start_time = train_data.shape[1] - pred_time_steps\n",
    "    # dense_mat0 = dense_mat[:, 0: start_time]\n",
    "    train_data0 = train_data[:, 0: start_time]\n",
    "    dim1 = train_data0.shape[0]\n",
    "    dim2 = train_data0.shape[1]\n",
    "    max_time_lag = max(time_lags)\n",
    "    results = {step + 1: np.zeros((dim1, pred_time_steps)) for step in range(multi_steps)}\n",
    "\n",
    "    for t in range(pred_time_steps):\n",
    "        if t == 0:\n",
    "            init = {\"W\": 0.1 * np.random.rand(dim1, rank), \"X\": 0.1 * np.random.rand(dim2, rank),\n",
    "                    \"theta\": 0.1 * np.random.rand(time_lags.shape[0], rank)}\n",
    "            W, X, theta, mat_f = TRMF(train_data0, init, time_lags,\n",
    "                                      lambda_w, lambda_x, lambda_theta, eta, maxiter, multi_steps, display=display)\n",
    "            # Assign forecast to the corresponding step\n",
    "            for step in range(multi_steps):\n",
    "                results[step + 1][:, t] = mat_f[:, step]\n",
    "            X0 = X[dim2 - max_time_lag:dim2 + 1, :].copy()  # Keep recent max_time_lag + one-step forecast\n",
    "        else:\n",
    "            sparse_vec = train_data[:, start_time + t - 1]\n",
    "            if np.where(sparse_vec > 0)[0].shape[0] > rank:\n",
    "                init = {\"W\": W, \"X\": X0, \"theta\": theta}\n",
    "                X = OnlineTRMF(sparse_vec, init, lambda_x / dim2, time_lags)\n",
    "                X0 = np.zeros((max_time_lag + multi_steps, rank))\n",
    "                X0[0: max_time_lag, :] = X[1:, :]\n",
    "                for step in range(multi_steps):\n",
    "                    step_X = np.einsum('ij, ij -> j', theta, X0[max_time_lag + step - time_lags, :])\n",
    "                    X0[max_time_lag + step, :] = step_X\n",
    "                    results[step + 1][:, t] = W @ step_X\n",
    "                X0 = X0[:max_time_lag + 1, :]  # Keep recent max_time_lag + one-step forecast\n",
    "            else:\n",
    "                X = X0.copy()\n",
    "                X0 = np.zeros((max_time_lag + multi_steps, rank))\n",
    "                X0[0: max_time_lag, :] = X[1:, :]\n",
    "                for step in range(multi_steps):\n",
    "                    step_X = np.einsum('ij, ij -> j', theta, X0[max_time_lag + step - time_lags, :])\n",
    "                    X0[max_time_lag + step, :] = step_X\n",
    "                    results[step + 1][:, t] = W @ step_X\n",
    "                X0 = X0[:max_time_lag + 1, :]  # Keep recent max_time_lag + one-step forecast\n",
    "\n",
    "        if (t + 1) % 40 == 0:\n",
    "            print('Time step: {}, time {}'.format(t + 1, time.time() - start))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "data0 = loadmat('..//data//OD_3m.mat')\n",
    "data0 = data0['OD']\n",
    "data0 = remove_weekends(data0, start=5)\n",
    "\n",
    "train_idx = start_end_idx('2017-07-03', '2017-08-11', weekend=False, night=False)\n",
    "test_idx = start_end_idx('2017-08-14', '2017-08-25', weekend=False, night=False)\n",
    "num_s = 159\n",
    "\n",
    "# Subtract the mean in the training set\n",
    "data = data0.astype(np.float64)\n",
    "data_mean = data[:, train_idx].reshape([num_s * num_s, 36, -1], order='F')\n",
    "data_mean = data_mean.mean(axis=2)\n",
    "for i in range(65):\n",
    "    data[:, i * 36:(i + 1) * 36] = data[:, i * 36:(i + 1) * 36] - data_mean"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Parameter tuning\n",
    "# Tune weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step: 100 time 422.6742262840271\n",
      "Time step: 200 time 833.2179036140442\n",
      "Time step: 300 time 1277.8192734718323\n",
      "Time step: 40, time 1292.9665188789368\n",
      "Time step: 80, time 1303.8736522197723\n",
      "Time step: 120, time 1314.7348022460938\n",
      "Time step: 160, time 1325.6314294338226\n",
      "Time step: 200, time 1336.5621950626373\n",
      "Time step: 240, time 1347.5160694122314\n",
      "Time step: 280, time 1358.4220051765442\n",
      "Time step: 320, time 1369.421989440918\n",
      "Time step: 360, time 1380.2815067768097\n",
      "[3.16668972973313]\n",
      "Time step: 100 time 444.49519443511963\n",
      "Time step: 200 time 893.9184141159058\n",
      "Time step: 300 time 1348.5971925258636\n",
      "Time step: 40, time 1364.0833823680878\n",
      "Time step: 80, time 1375.4222617149353\n",
      "Time step: 120, time 1386.5361912250519\n",
      "Time step: 160, time 1397.5827927589417\n",
      "Time step: 200, time 1408.5984964370728\n",
      "Time step: 240, time 1419.7393562793732\n",
      "Time step: 280, time 1431.2546632289886\n",
      "Time step: 320, time 1443.195256471634\n",
      "Time step: 360, time 1454.570856332779\n",
      "[3.16668972973313, 3.0305339309035264]\n",
      "Time step: 100 time 449.30922389030457\n",
      "Time step: 200 time 898.5096545219421\n",
      "Time step: 300 time 1222.1612031459808\n",
      "Time step: 40, time 1233.785961151123\n",
      "Time step: 80, time 1242.208392381668\n",
      "Time step: 120, time 1250.6626133918762\n",
      "Time step: 160, time 1259.113848209381\n",
      "Time step: 200, time 1267.5830252170563\n",
      "Time step: 240, time 1276.0840759277344\n",
      "Time step: 280, time 1284.4754354953766\n",
      "Time step: 320, time 1292.8976600170135\n",
      "Time step: 360, time 1301.3354923725128\n",
      "[3.16668972973313, 3.0305339309035264, 2.9609771329565566]\n",
      "Time step: 100 time 306.4598367214203\n",
      "Time step: 200 time 616.8182573318481\n",
      "Time step: 300 time 925.9557592868805\n",
      "Time step: 40, time 937.3155300617218\n",
      "Time step: 80, time 945.8343341350555\n",
      "Time step: 120, time 954.224157333374\n",
      "Time step: 160, time 962.5204575061798\n",
      "Time step: 200, time 970.8967463970184\n",
      "Time step: 240, time 979.4603404998779\n",
      "Time step: 280, time 987.9096903800964\n",
      "Time step: 320, time 996.6151220798492\n",
      "Time step: 360, time 1005.1625719070435\n",
      "[3.16668972973313, 3.0305339309035264, 2.9609771329565566, 2.896872444818558]\n",
      "Time step: 100 time 310.240788936615\n",
      "Time step: 200 time 620.5599799156189\n",
      "Time step: 300 time 929.3845303058624\n",
      "Time step: 40, time 940.6191117763519\n",
      "Time step: 80, time 949.088103055954\n",
      "Time step: 120, time 957.4949383735657\n",
      "Time step: 160, time 965.9327292442322\n",
      "Time step: 200, time 974.3551006317139\n",
      "Time step: 240, time 982.9187531471252\n",
      "Time step: 280, time 991.309757232666\n",
      "Time step: 320, time 999.7922306060791\n",
      "Time step: 360, time 1008.2145895957947\n",
      "[3.16668972973313, 3.0305339309035264, 2.9609771329565566, 2.896872444818558, 2.882051729829047]\n",
      "Time step: 100 time 306.8946304321289\n",
      "Time step: 200 time 615.5151655673981\n",
      "Time step: 300 time 923.4953496456146\n",
      "Time step: 40, time 934.8082644939423\n",
      "Time step: 80, time 943.2621359825134\n",
      "Time step: 120, time 951.6379652023315\n",
      "Time step: 160, time 960.0769689083099\n",
      "Time step: 200, time 968.9343531131744\n",
      "Time step: 240, time 977.3719100952148\n",
      "Time step: 280, time 985.7475152015686\n",
      "Time step: 320, time 994.2954936027527\n",
      "Time step: 360, time 1002.6397757530212\n",
      "[3.16668972973313, 3.0305339309035264, 2.9609771329565566, 2.896872444818558, 2.882051729829047, 2.8735854036377777]\n",
      "Time step: 100 time 312.39979362487793\n",
      "Time step: 200 time 627.3307309150696\n",
      "Time step: 300 time 950.1787571907043\n",
      "Time step: 40, time 961.8818006515503\n",
      "Time step: 80, time 970.3177042007446\n",
      "Time step: 120, time 978.819885969162\n",
      "Time step: 160, time 987.3051965236664\n",
      "Time step: 200, time 995.7876582145691\n",
      "Time step: 240, time 1004.3349916934967\n",
      "Time step: 280, time 1012.7103669643402\n",
      "Time step: 320, time 1021.1178617477417\n",
      "Time step: 360, time 1029.6176552772522\n",
      "[3.16668972973313, 3.0305339309035264, 2.9609771329565566, 2.896872444818558, 2.882051729829047, 2.8735854036377777, 2.8744220293366194]\n",
      "Time step: 100 time 307.7942144870758\n",
      "Time step: 200 time 658.0024607181549\n",
      "Time step: 300 time 985.8138909339905\n",
      "Time step: 40, time 997.2070400714874\n",
      "Time step: 80, time 1005.7363684177399\n",
      "Time step: 120, time 1014.1118674278259\n",
      "Time step: 160, time 1022.5965175628662\n",
      "Time step: 200, time 1031.2219331264496\n",
      "Time step: 240, time 1039.5819897651672\n",
      "Time step: 280, time 1048.0042643547058\n",
      "Time step: 320, time 1056.3175134658813\n",
      "Time step: 360, time 1064.7737810611725\n",
      "[3.16668972973313, 3.0305339309035264, 2.9609771329565566, 2.896872444818558, 2.882051729829047, 2.8735854036377777, 2.8744220293366194, 2.91393771154424]\n",
      "Time step: 100 time 307.42754554748535\n",
      "Time step: 200 time 658.2088391780853\n",
      "Time step: 300 time 1442.7511756420135\n",
      "Time step: 40, time 1562.410575389862\n",
      "Time step: 80, time 1597.457133769989\n",
      "Time step: 120, time 1632.7564465999603\n",
      "Time step: 160, time 1669.3677291870117\n",
      "Time step: 200, time 1705.7737188339233\n",
      "Time step: 240, time 1742.3224442005157\n",
      "Time step: 280, time 1778.8705897331238\n",
      "Time step: 320, time 1815.322497844696\n",
      "Time step: 360, time 1851.8858807086945\n",
      "[3.16668972973313, 3.0305339309035264, 2.9609771329565566, 2.896872444818558, 2.882051729829047, 2.8735854036377777, 2.8744220293366194, 2.91393771154424, 2.91393771154424]\n",
      "best_weight is 6000\n"
     ]
    }
   ],
   "source": [
    "multi_steps = 1\n",
    "pred_time_steps = 36 * 10 + (multi_steps - 1)\n",
    "train_data = data[:, train_idx]\n",
    "time_lags = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "d = time_lags.shape[0]\n",
    "maxiter = 300\n",
    "\n",
    "# Tune weights\n",
    "eta = 0.03\n",
    "rank = 40\n",
    "rmse_list = []\n",
    "weights = range(1000, 10000, 1000)\n",
    "reset_random_seeds(1)\n",
    "for weight in weights:\n",
    "    lambda_w = weight\n",
    "    lambda_x = weight\n",
    "    lambda_theta = weight\n",
    "    results = st_prediction(train_data, time_lags, lambda_w, lambda_x, lambda_theta,\n",
    "                            eta, rank, pred_time_steps, maxiter, multi_steps, display=100)\n",
    "    rmse_list.append(RMSE(train_data[:, -36 * 10:], results[1]))\n",
    "    print(rmse_list)\n",
    "\n",
    "best_weight = weights[np.argmin(rmse_list)]\n",
    "print('best_weight is {}'.format(best_weight))  # was 3000"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tune rank"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step: 100 time 131.31524014472961\n",
      "Time step: 200 time 265.184494972229\n",
      "Time step: 300 time 398.04875469207764\n",
      "Time step: 40, time 401.73542189598083\n",
      "Time step: 80, time 404.2212748527527\n",
      "Time step: 120, time 406.5488770008087\n",
      "Time step: 160, time 408.8296117782593\n",
      "Time step: 200, time 411.28218030929565\n",
      "Time step: 240, time 413.61249351501465\n",
      "Time step: 280, time 415.8463637828827\n",
      "Time step: 320, time 418.21504759788513\n",
      "Time step: 360, time 420.62076210975647\n",
      "[2.8805232149000175]\n",
      "Time step: 100 time 167.7458872795105\n",
      "Time step: 200 time 329.2378520965576\n",
      "Time step: 300 time 484.9009590148926\n",
      "Time step: 40, time 489.8233585357666\n",
      "Time step: 80, time 493.1829447746277\n",
      "Time step: 120, time 496.5103235244751\n",
      "Time step: 160, time 499.8064503669739\n",
      "Time step: 200, time 503.27711749076843\n",
      "Time step: 240, time 506.5420072078705\n",
      "Time step: 280, time 509.9474878311157\n",
      "Time step: 320, time 513.2148325443268\n",
      "Time step: 360, time 516.5734543800354\n",
      "[2.8805232149000175, 2.8772345682025073]\n",
      "Time step: 100 time 180.7686619758606\n",
      "Time step: 200 time 362.9424946308136\n",
      "Time step: 300 time 546.1491680145264\n",
      "Time step: 40, time 552.6977546215057\n",
      "Time step: 80, time 557.540388584137\n",
      "Time step: 120, time 562.222448348999\n",
      "Time step: 160, time 567.033858537674\n",
      "Time step: 200, time 571.7607967853546\n",
      "Time step: 240, time 576.5253472328186\n",
      "Time step: 280, time 581.2924766540527\n",
      "Time step: 320, time 586.0414052009583\n",
      "Time step: 360, time 590.9487919807434\n",
      "[2.8805232149000175, 2.8772345682025073, 2.875977562340926]\n",
      "Time step: 100 time 269.27242851257324\n",
      "Time step: 200 time 539.5602021217346\n",
      "Time step: 300 time 808.8605260848999\n",
      "Time step: 40, time 818.033289194107\n",
      "Time step: 80, time 824.452271938324\n",
      "Time step: 120, time 830.9428505897522\n",
      "Time step: 160, time 837.2851595878601\n",
      "Time step: 200, time 843.8783936500549\n",
      "Time step: 240, time 850.4271476268768\n",
      "Time step: 280, time 857.0350291728973\n",
      "Time step: 320, time 863.457300901413\n",
      "Time step: 360, time 870.1150901317596\n",
      "[2.8805232149000175, 2.8772345682025073, 2.875977562340926, 2.872610371383922]\n",
      "Time step: 100 time 307.81724214553833\n",
      "Time step: 200 time 617.6911215782166\n",
      "Time step: 300 time 928.0781102180481\n",
      "Time step: 40, time 939.6562659740448\n",
      "Time step: 80, time 948.2812089920044\n",
      "Time step: 120, time 956.6855573654175\n",
      "Time step: 160, time 965.1548719406128\n",
      "Time step: 200, time 973.62420129776\n",
      "Time step: 240, time 982.047324180603\n",
      "Time step: 280, time 990.4380660057068\n",
      "Time step: 320, time 998.8452062606812\n",
      "Time step: 360, time 1007.3296830654144\n",
      "[2.8805232149000175, 2.8772345682025073, 2.875977562340926, 2.872610371383922, 2.872913645078683]\n",
      "Time step: 100 time 357.6160218715668\n",
      "Time step: 200 time 717.8818860054016\n",
      "Time step: 300 time 1078.1594870090485\n",
      "Time step: 40, time 1092.1074888706207\n",
      "Time step: 80, time 1102.8496825695038\n",
      "Time step: 120, time 1113.6390445232391\n",
      "Time step: 160, time 1124.3574163913727\n",
      "Time step: 200, time 1135.1851909160614\n",
      "Time step: 240, time 1145.7322912216187\n",
      "Time step: 280, time 1156.529453277588\n",
      "Time step: 320, time 1167.2327828407288\n",
      "Time step: 360, time 1177.9358036518097\n",
      "[2.8805232149000175, 2.8772345682025073, 2.875977562340926, 2.872610371383922, 2.872913645078683, 2.875402624101292]\n",
      "Time step: 100 time 409.27086448669434\n",
      "Time step: 200 time 821.1887228488922\n",
      "Time step: 300 time 1233.558173418045\n",
      "Time step: 40, time 1250.6066672801971\n",
      "Time step: 80, time 1263.7774438858032\n",
      "Time step: 120, time 1276.9341921806335\n",
      "Time step: 160, time 1289.9196848869324\n",
      "Time step: 200, time 1302.982319355011\n",
      "Time step: 240, time 1316.0905244350433\n",
      "Time step: 280, time 1329.1416127681732\n",
      "Time step: 320, time 1342.28035902977\n",
      "Time step: 360, time 1355.3900926113129\n",
      "[2.8805232149000175, 2.8772345682025073, 2.875977562340926, 2.872610371383922, 2.872913645078683, 2.875402624101292, 2.8727363354841033]\n",
      "Time step: 100 time 466.67549204826355\n",
      "Time step: 200 time 933.0870361328125\n",
      "Time step: 300 time 1401.8873901367188\n",
      "Time step: 40, time 1421.9655442237854\n",
      "Time step: 80, time 1437.74933385849\n",
      "Time step: 120, time 1453.5134499073029\n",
      "Time step: 160, time 1469.327400445938\n",
      "Time step: 200, time 1485.216480731964\n",
      "Time step: 240, time 1500.9373786449432\n",
      "Time step: 280, time 1516.8603279590607\n",
      "Time step: 320, time 1532.6254878044128\n",
      "Time step: 360, time 1548.517501592636\n",
      "[2.8805232149000175, 2.8772345682025073, 2.875977562340926, 2.872610371383922, 2.872913645078683, 2.875402624101292, 2.8727363354841033, 2.8769984416516845]\n",
      "Time step: 100 time 513.0227196216583\n",
      "Time step: 200 time 1033.576688528061\n",
      "Time step: 300 time 1554.7005822658539\n",
      "Time step: 40, time 1578.169783115387\n",
      "Time step: 80, time 1596.90376162529\n",
      "Time step: 120, time 1615.6394670009613\n",
      "Time step: 160, time 1634.580049753189\n",
      "Time step: 200, time 1653.1255791187286\n",
      "Time step: 240, time 1671.938193321228\n",
      "Time step: 280, time 1690.5491499900818\n",
      "Time step: 320, time 1709.2995400428772\n",
      "Time step: 360, time 1727.911615371704\n",
      "[2.8805232149000175, 2.8772345682025073, 2.875977562340926, 2.872610371383922, 2.872913645078683, 2.875402624101292, 2.8727363354841033, 2.8769984416516845, 2.8720241824584263]\n",
      "best_rank is 60\n"
     ]
    }
   ],
   "source": [
    "lambda_w = best_weight\n",
    "lambda_x = best_weight\n",
    "lambda_theta = best_weight\n",
    "rmse_list = []\n",
    "ranks = range(20, 65, 5)\n",
    "reset_random_seeds(1)\n",
    "for rank in ranks:\n",
    "    results = st_prediction(train_data, time_lags, lambda_w, lambda_x, lambda_theta,\n",
    "                            eta, rank, pred_time_steps, maxiter, multi_steps)\n",
    "    rmse_list.append(RMSE(train_data[:, -36 * 10:], results[1]))\n",
    "    print(rmse_list)\n",
    "\n",
    "best_rank = ranks[np.argmin(rmse_list)]\n",
    "print(\"best_rank is {}\".format(best_rank))  # was 35"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Forcast and save results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step: 100 time 415.1035301685333\n",
      "Time step: 200 time 835.7077527046204\n",
      "Time step: 300 time 1252.8783340454102\n",
      "Time step: 40, time 1267.6778545379639\n",
      "Time step: 80, time 1278.3648753166199\n",
      "Time step: 120, time 1289.1926267147064\n",
      "Time step: 160, time 1299.974618434906\n",
      "Time step: 200, time 1310.7396836280823\n",
      "Time step: 240, time 1321.4582805633545\n",
      "Time step: 280, time 1332.1303339004517\n",
      "Time step: 320, time 1342.974592924118\n",
      "Time step: 360, time 1353.759488582611\n",
      "Results of 1-step forecasting:\n",
      "RMSE of OD: 3.1684082205630237\n",
      "WMAPE of OD: 0.30079949227553543\n",
      "SMAPE of OD: 1.0784552098939733\n",
      "MAE of OD: 1.5284633566432184\n",
      "r2 of OD: 0.9532032169118612\n",
      "\n",
      "\n",
      "RMSE of flow: 104.5999755859375\n",
      "WMAPE of flow: 0.06674592941999435\n",
      "SMAPE of flow: 0.14253036677837372\n",
      "MAE of flow: 53.926204681396484\n",
      "r2 of flow: 0.9884410497556592\n",
      "Results of 2-step forecasting:\n",
      "RMSE of OD: 3.273830691388082\n",
      "WMAPE of OD: 0.30897029365243134\n",
      "SMAPE of OD: 1.083369958543836\n",
      "MAE of OD: 1.5699819456691453\n",
      "r2 of OD: 0.9500372690798786\n",
      "\n",
      "\n",
      "RMSE of flow: 122.11442565917969\n",
      "WMAPE of flow: 0.07844962924718857\n",
      "SMAPE of flow: 0.10602400451898575\n",
      "MAE of flow: 63.38200378417969\n",
      "r2 of flow: 0.9842460590004717\n",
      "Results of 3-step forecasting:\n",
      "RMSE of OD: 3.4251056668368305\n",
      "WMAPE of OD: 0.31904623399252413\n",
      "SMAPE of OD: 1.0865909618200256\n",
      "MAE of OD: 1.621181186322942\n",
      "r2 of OD: 0.9453133039853338\n",
      "\n",
      "\n",
      "RMSE of flow: 143.0567626953125\n",
      "WMAPE of flow: 0.09121241420507431\n",
      "SMAPE of flow: 0.250760942697525\n",
      "MAE of flow: 73.69347381591797\n",
      "r2 of flow: 0.9783791857087113\n"
     ]
    }
   ],
   "source": [
    "best_rank = 45\n",
    "best_weight = 6000\n",
    "multi_steps = 3\n",
    "pred_time_steps = 36 * 10 + (multi_steps - 1)\n",
    "train_data = data[:, np.concatenate([train_idx, test_idx])]\n",
    "time_lags = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "rank = best_rank\n",
    "lambda_w = best_weight\n",
    "lambda_x = best_weight\n",
    "lambda_theta = best_weight\n",
    "eta = 0.03\n",
    "\n",
    "maxiter = 300\n",
    "reset_random_seeds(1)\n",
    "results = st_prediction(train_data, time_lags, lambda_w, lambda_x, lambda_theta,\n",
    "                        eta, rank, pred_time_steps, maxiter, multi_steps)\n",
    "\n",
    "mat_hat1 = results[1][:, 2:2 + 360].copy()\n",
    "mat_hat2 = results[2][:, 1:1 + 360].copy()\n",
    "mat_hat3 = results[3][:, 0:0 + 360].copy()\n",
    "for i in range(mat_hat1.shape[1]):\n",
    "    mat_hat1[:, i] += data_mean[:, i % 36]\n",
    "    mat_hat2[:, i] += data_mean[:, i % 36]\n",
    "    mat_hat3[:, i] += data_mean[:, i % 36]\n",
    "\n",
    "real_OD = data0[:, test_idx]\n",
    "real_flow = od2flow(real_OD, num_s=num_s)\n",
    "print('Results of 1-step forecasting:')\n",
    "predict_flow1 = od2flow(mat_hat1, num_s=num_s)\n",
    "get_score(real_OD, mat_hat1, real_flow, predict_flow1)\n",
    "\n",
    "print('Results of 2-step forecasting:')\n",
    "predict_flow2 = od2flow(mat_hat2, num_s=num_s)\n",
    "get_score(real_OD, mat_hat2, real_flow, predict_flow2)\n",
    "\n",
    "print('Results of 3-step forecasting:')\n",
    "predict_flow3 = od2flow(mat_hat3, num_s=num_s)\n",
    "get_score(real_OD, mat_hat3, real_flow, predict_flow3)\n",
    "\n",
    "np.savez_compressed('..//data//Guangzhou_OD_TRMF_step1.npz', data=mat_hat1)\n",
    "np.savez_compressed('..//data//Guangzhou_OD_TRMF_step2.npz', data=mat_hat2)\n",
    "np.savez_compressed('..//data//Guangzhou_OD_TRMF_step3.npz', data=mat_hat3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}