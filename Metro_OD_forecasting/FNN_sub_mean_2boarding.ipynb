{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FNN_sub_mean_2boarding.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hP1Vw220XMaL",
        "outputId": "451beb9d-85fa-4682-a181-129de93abc18"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=1418.4008>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eucRnBArDxBt",
        "outputId": "08b3bb35-2b1e-4800-a0d7-7a9a5daf3dd5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZXCbaZLXZrz"
      },
      "source": [
        "import matplotlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from scipy.io import loadmat\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import gc\n",
        "tf.keras.backend.set_floatx('float64')\n",
        "\n",
        "def stagger_data(data, h):\n",
        "    \"\"\"\n",
        "    >>> i = np.array([[1, 2, 3, 4, 5, 6], [7, 8, 9, 10, 11, 12]])\n",
        "    >>> stagger_data(i, [1, 3])\n",
        "    (array([[ 3,  4,  5],\n",
        "           [ 9, 10, 11],\n",
        "           [ 1,  2,  3],\n",
        "           [ 7,  8,  9]]), array([[ 4,  5,  6],\n",
        "           [10, 11, 12]]))\n",
        "    \"\"\"\n",
        "    h.sort()\n",
        "    len_h = len(h)\n",
        "    n, m = data.shape\n",
        "    max_h = max(h)\n",
        "\n",
        "    Y = data[:, max_h:]\n",
        "    X = np.zeros((n * len_h, m - max_h), dtype=data.dtype)\n",
        "    for i in range(len_h):\n",
        "        X[i * n: i * n + n, :] = data[:, max_h - h[i]:m - h[i]]\n",
        "    return X, Y\n",
        "\n",
        "\n",
        "def remove_weekends(data, start=0, bs=36):\n",
        "    _, m = data.shape\n",
        "    n_day = int(m / bs)\n",
        "    weekday = np.concatenate([np.arange(start, 7) % 7, np.arange(n_day) % 7])[:n_day]\n",
        "    weekday = np.repeat(weekday, bs)\n",
        "    return data[:, weekday < 5]\n",
        "\n",
        "\n",
        "def get_flow1(od, s, dir='o'):\n",
        "    \"\"\"Get the flow of station `s`\"\"\"\n",
        "    n = od.shape[0]\n",
        "    if dir == 'o':\n",
        "        idx = np.arange(s, n, 159)\n",
        "    elif dir == 'd':\n",
        "        idx = np.arange((s * 159), (s * 159 + 159))\n",
        "    return np.sum(od[idx, :], axis=0)\n",
        "\n",
        "\n",
        "def od2flow(od, s_list=None, dir='o'):\n",
        "    if s_list is None:\n",
        "        s_list = range(159)\n",
        "\n",
        "    n_s = len(s_list)\n",
        "    flow = np.zeros((n_s, od.shape[1]), dtype=np.float32)\n",
        "    for i, s in enumerate(s_list):\n",
        "        flow[i, :] = get_flow1(od, s, dir)\n",
        "    return flow\n",
        "\n",
        "\n",
        "def RMSE(f0, f1, axis=None):\n",
        "    return np.sqrt(np.mean((f0 - f1) ** 2, axis))\n",
        "  \n",
        "\n",
        "def start_end_idx(start, end, weekend=False, night=False):\n",
        "    date = pd.period_range('2017-07-01', '2017-09-30 23:30', freq='30T')\n",
        "    date = date.to_timestamp()\n",
        "    if not night:\n",
        "        date = date[date.hour >= 6]\n",
        "    if not weekend:\n",
        "        date = date[date.weekday < 5]\n",
        "    idx = pd.DataFrame(data=np.arange(date.shape[0]), index=date)\n",
        "    return idx.loc[start:end, :].values.ravel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KzTgYUw4Uez"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ts5me2dHS9JS"
      },
      "source": [
        "data0 = loadmat('drive//MyDrive//data//OD_3m.mat')\n",
        "data0 = data0['OD']\n",
        "data0 = remove_weekends(data0, start=5)\n",
        "\n",
        "# Subtract the mean of the training set\n",
        "data = data0.astype(np.float64)\n",
        "data_mean = data[:, 0:20*36].reshape([159*159, 36, -1], order='F')\n",
        "data_mean = data_mean.mean(axis=2)\n",
        "for i in range(65):\n",
        "    data[:,i*36:(i+1)*36] = data[:,i*36:(i+1)*36] - data_mean\n",
        "\n",
        "train_idx = start_end_idx('2017-07-03', '2017-07-28', weekend=False, night=False)\n",
        "validate_idx = start_end_idx('2017-07-31', '2017-08-11', weekend=False, night=False)\n",
        "test_idx = start_end_idx('2017-08-14', '2017-08-25', weekend=False, night=False)\n",
        "\n",
        "flow0 = od2flow(data)\n",
        "flow = np.zeros((flow0.shape[0]*2, flow0.shape[1]), dtype=flow0.dtype)\n",
        "flow[0:flow0.shape[0], :] = flow0\n",
        "flow[flow0.shape[0]:, 1:] = flow0[:, 0:-1]\n",
        "\n",
        "h = [3, 4, 5, 6, 7, 8, 9, 10]\n",
        "X_train, Y_train = stagger_data(data[:, train_idx], h)\n",
        "m_train = X_train.shape[1]\n",
        "X_train = np.concatenate([X_train, flow[:, train_idx][:, -m_train-1:-1]/159]).T\n",
        "Y_train = Y_train.T\n",
        "\n",
        "X_validate, Y_validate = stagger_data(data[:, np.concatenate([train_idx[-max(h):], validate_idx])], h)\n",
        "X_validate = np.concatenate([X_validate, flow[:, validate_idx-1]/159]).T\n",
        "Y_validate = Y_validate.T\n",
        "\n",
        "X_test, Y_test = stagger_data(data[:, np.concatenate([validate_idx[-max(h):], test_idx])], h)\n",
        "X_test = np.concatenate([X_test, flow[:, test_idx-1]/159]).T\n",
        "Y_test = Y_test.T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuWHNahF4b5f"
      },
      "source": [
        "# Select model order"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RnJ5Jq5lf2_"
      },
      "source": [
        "def create_net(n_input=159*159*10, n_hidden=50, activation=None):\n",
        "  seq = keras.Sequential(\n",
        "      [\n",
        "      layers.Dense(n_hidden, input_shape=(n_input,), \n",
        "                   activation=activation,\n",
        "                  #  kernel_regularizer=tf.keras.regularizers.L2(0.0001)\n",
        "                   ),\n",
        "      layers.Dense(159*159,\n",
        "                  #  kernel_regularizer=tf.keras.regularizers.L2(0.0001)\n",
        "                   )\n",
        "      ]\n",
        "  )\n",
        "  return seq\n",
        "\n",
        "call_back = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', min_delta=0, patience=20, verbose=0, mode='auto',\n",
        "    baseline=None, restore_best_weights=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-38tw9yYtVW",
        "outputId": "866fdf7d-56f1-4cbd-e330-7fbe81a4e777"
      },
      "source": [
        "activation_list = ['linear', 'sigmoid', 'relu']\n",
        "n_hidden_list = np.linspace(10, 100, 10, dtype=np.int)\n",
        "best_e = 10000\n",
        "\n",
        "for activation in activation_list:\n",
        "    for n_hidden in n_hidden_list:\n",
        "        tf.keras.backend.clear_session()\n",
        "        tf.random.set_seed(1)\n",
        "        seq = create_net(n_input=X_train.shape[1], n_hidden=n_hidden, activation=activation)\n",
        "        gc.collect()\n",
        "        seq.compile(loss=\"mean_squared_error\", optimizer=\"RMSprop\")\n",
        "        seq.fit(\n",
        "            x=X_train,\n",
        "            y=Y_train,\n",
        "            batch_size=32,\n",
        "            epochs=200,\n",
        "            verbose=0,\n",
        "            shuffle=True,\n",
        "            validation_data=(X_validate, Y_validate),\n",
        "            callbacks=[call_back],\n",
        "        )\n",
        "        predict_validate = seq.predict(X_validate)\n",
        "        rmse = RMSE(predict_validate, Y_validate)\n",
        "\n",
        "        if rmse<best_e:\n",
        "            best_e = rmse\n",
        "            best_model = seq\n",
        "        print('Activation {} hidden_layers {}, RMSE {}, current best RMSE {}'.format(activation, n_hidden, rmse, best_e))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Activation linear hidden_layers 10, RMSE 3.0939109330214096, current best RMSE 3.0939109330214096\n",
            "Activation linear hidden_layers 20, RMSE 3.095054706964265, current best RMSE 3.0939109330214096\n",
            "Activation linear hidden_layers 30, RMSE 3.0936376910944134, current best RMSE 3.0936376910944134\n",
            "Activation linear hidden_layers 40, RMSE 3.0977290257856964, current best RMSE 3.0936376910944134\n",
            "Activation linear hidden_layers 50, RMSE 3.090493691030671, current best RMSE 3.090493691030671\n",
            "Activation linear hidden_layers 60, RMSE 3.0957928095770684, current best RMSE 3.090493691030671\n",
            "Activation linear hidden_layers 70, RMSE 3.092542771713564, current best RMSE 3.090493691030671\n",
            "Activation linear hidden_layers 80, RMSE 3.093912434060778, current best RMSE 3.090493691030671\n",
            "Activation linear hidden_layers 90, RMSE 3.0914988669704657, current best RMSE 3.090493691030671\n",
            "Activation linear hidden_layers 100, RMSE 3.1083641925081302, current best RMSE 3.090493691030671\n",
            "Activation sigmoid hidden_layers 10, RMSE 3.109881770695598, current best RMSE 3.090493691030671\n",
            "Activation sigmoid hidden_layers 20, RMSE 3.1105691023475055, current best RMSE 3.090493691030671\n",
            "Activation sigmoid hidden_layers 30, RMSE 3.109052065912548, current best RMSE 3.090493691030671\n",
            "Activation sigmoid hidden_layers 40, RMSE 3.10669019214708, current best RMSE 3.090493691030671\n",
            "Activation sigmoid hidden_layers 50, RMSE 3.106001962441592, current best RMSE 3.090493691030671\n",
            "Activation sigmoid hidden_layers 60, RMSE 3.1049706988936956, current best RMSE 3.090493691030671\n",
            "Activation sigmoid hidden_layers 70, RMSE 3.1056323183643757, current best RMSE 3.090493691030671\n",
            "Activation sigmoid hidden_layers 80, RMSE 3.105573432713467, current best RMSE 3.090493691030671\n",
            "Activation sigmoid hidden_layers 90, RMSE 3.102558863719995, current best RMSE 3.090493691030671\n",
            "Activation sigmoid hidden_layers 100, RMSE 3.101275118673458, current best RMSE 3.090493691030671\n",
            "Activation relu hidden_layers 10, RMSE 3.1069636528150686, current best RMSE 3.090493691030671\n",
            "Activation relu hidden_layers 20, RMSE 3.114159632506549, current best RMSE 3.090493691030671\n",
            "Activation relu hidden_layers 30, RMSE 3.113303317397589, current best RMSE 3.090493691030671\n",
            "Activation relu hidden_layers 40, RMSE 3.1232004786469645, current best RMSE 3.090493691030671\n",
            "Activation relu hidden_layers 50, RMSE 3.1254416935379763, current best RMSE 3.090493691030671\n",
            "Activation relu hidden_layers 60, RMSE 3.128194945305558, current best RMSE 3.090493691030671\n",
            "Activation relu hidden_layers 70, RMSE 3.1307968436713534, current best RMSE 3.090493691030671\n",
            "Activation relu hidden_layers 80, RMSE 3.1301131189490676, current best RMSE 3.090493691030671\n",
            "Activation relu hidden_layers 90, RMSE 3.1359504068246205, current best RMSE 3.090493691030671\n",
            "Activation relu hidden_layers 100, RMSE 3.135667856818994, current best RMSE 3.090493691030671\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgbG80DTDSks",
        "outputId": "196b9777-7131-40b3-9432-f024b3af79c4"
      },
      "source": [
        "# Retrain the model by train + validate set, using the best hyperparameters.\n",
        "X_train_new, Y_train_new = stagger_data(data[:, :test_idx[1]], h)\n",
        "m_train_new = X_train_new.shape[1]\n",
        "X_train_new = np.concatenate([X_train_new, flow[:, :test_idx[1]][:, -m_train_new-1:-1]/159]).T\n",
        "Y_train_new = Y_train_new.T\n",
        "\n",
        "# Train the model\n",
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(1)\n",
        "seq = create_net(n_input=X_train_new.shape[1], n_hidden=50, activation='linear')\n",
        "gc.collect()\n",
        "seq.compile(loss=\"mean_squared_error\", optimizer=\"RMSprop\")\n",
        "seq.fit(\n",
        "    x=X_train_new,\n",
        "    y=Y_train_new,\n",
        "    batch_size=32,\n",
        "    epochs=200,\n",
        "    verbose=0,\n",
        "    shuffle=True,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[call_back],\n",
        ")\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "gc.collect()\n",
        "\n",
        "seq.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 50)                10128350  \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 25281)             1289331   \n",
            "=================================================================\n",
            "Total params: 11,417,681\n",
            "Trainable params: 11,417,681\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8MsxO1f-sO4"
      },
      "source": [
        "# Multistep forecast for the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlwFgpsyqVrC",
        "outputId": "6e941aa0-1a7a-43ed-c671-83f75941352a"
      },
      "source": [
        "# One-step forecast\r\n",
        "result1 = seq.predict(np.concatenate([X_validate, X_test], axis=0))\r\n",
        "tf.keras.backend.clear_session()\r\n",
        "gc.collect()\r\n",
        "\r\n",
        "print('RMSE on test data: {}'.format(RMSE(np.concatenate([Y_validate, Y_test], axis=0)[360:, :], result1[360:, :])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE on test data: 3.2557383521303676\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-4i8vG1Ecy6",
        "outputId": "25e8a909-8616-40d1-9979-2d7f2b7ad3fc"
      },
      "source": [
        "# Two-step forecast\r\n",
        "tf.keras.backend.clear_session()\r\n",
        "gc.collect()\r\n",
        "n = data.shape[0]\r\n",
        "nh = len(h)\r\n",
        "X2 = np.concatenate([X_validate, X_test])\r\n",
        "\r\n",
        "# Reuse one-step forecast OD\r\n",
        "X2[3:, 0:n] = result1[0:-3, :]\r\n",
        "\r\n",
        "# Reuse one-step forecast flow\r\n",
        "X2[3:, n*nh:n*nh+159] = od2flow(result1[2:-1, :].T).T/159\r\n",
        "\r\n",
        "# Forecast\r\n",
        "result2 = seq.predict(X2)\r\n",
        "tf.keras.backend.clear_session()\r\n",
        "gc.collect()\r\n",
        "\r\n",
        "print('RMSE on test data: {}'.format(RMSE(np.concatenate([Y_validate, Y_test], axis=0)[360:, :], result2[360:, :])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE on test data: 3.274448125602289\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ij-DWpx9Bf4",
        "outputId": "c8e713a6-6e64-48a3-87dd-ba0af44f7e22"
      },
      "source": [
        "# Three-step forecast\r\n",
        "n = data.shape[0]\r\n",
        "nh = len(h)\r\n",
        "X3 = np.concatenate([X_validate, X_test])\r\n",
        "# Reuse one-step forecast OD\r\n",
        "X3[4:, 0:n] = result1[1:-3, :]\r\n",
        "X3[4:, n:2*n] = result1[0:-4, :]\r\n",
        "\r\n",
        "# Reuse one-step forecast flow\r\n",
        "X3[4:, n*nh:n*nh+159] = od2flow(result1[3:-1, :].T).T/159\r\n",
        "X3[4:, n*nh+159:] = od2flow(result1[2:-2, :].T).T/159\r\n",
        "\r\n",
        "# Forecast\r\n",
        "result3 = seq.predict(X3)\r\n",
        "tf.keras.backend.clear_session()\r\n",
        "gc.collect()\r\n",
        "\r\n",
        "print('RMSE on test data: {}'.format(RMSE(np.concatenate([Y_validate, Y_test], axis=0)[360:, :], result3[360:, :])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE on test data: 3.2855431714645014\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yV2OOG8PlVz"
      },
      "source": [
        "# Add mean back and save results to file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c677160QNGVr"
      },
      "source": [
        "result1=result1[360:, :].T\r\n",
        "for i in range(result1.shape[1]):\r\n",
        "  result1[:,i] += data_mean[:, i%36]\r\n",
        "\r\n",
        "result2=result2[360:, :].T\r\n",
        "for i in range(result2.shape[1]):\r\n",
        "  result2[:,i] += data_mean[:, i%36]\r\n",
        "\r\n",
        "result3=result3[360:, :].T\r\n",
        "for i in range(result3.shape[1]):\r\n",
        "  result3[:,i] += data_mean[:, i%36]\r\n",
        "\r\n",
        "np.savez_compressed('/content/drive/MyDrive/data/OD_FNN_step1.npz', data=result1)\r\n",
        "np.savez_compressed('/content/drive/MyDrive/data/OD_FNN_step2.npz', data=result2)\r\n",
        "np.savez_compressed('/content/drive/MyDrive/data/OD_FNN_step3.npz', data=result3)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}