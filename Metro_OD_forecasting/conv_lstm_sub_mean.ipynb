{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "conv_lstm_sub_mean.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hP1Vw220XMaL",
        "outputId": "20871beb-eae3-48ef-db63-6d646c68addf"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=-2901.7256>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdeeYv2hQBJJ",
        "outputId": "ad7ac90c-9440-4e9d-ee3d-fd5a28b0735c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZXCbaZLXZrz"
      },
      "source": [
        "import tensorflow as tf\n",
        "import timeit\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from scipy.io import loadmat\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import gc\n",
        "\n",
        "\n",
        "def stagger_data(data, h):\n",
        "    \"\"\"\n",
        "    >>> i = np.array([[1, 2, 3, 4, 5, 6], [7, 8, 9, 10, 11, 12]])\n",
        "    >>> stagger_data(i, [1, 3])\n",
        "    (array([[ 3,  4,  5],\n",
        "           [ 9, 10, 11],\n",
        "           [ 1,  2,  3],\n",
        "           [ 7,  8,  9]]), array([[ 4,  5,  6],\n",
        "           [10, 11, 12]]))\n",
        "    \"\"\"\n",
        "    h.sort()\n",
        "    len_h = len(h)\n",
        "    n, m = data.shape\n",
        "    max_h = max(h)\n",
        "\n",
        "    Y = data[:, max_h:]\n",
        "    X = np.zeros((n * len_h, m - max_h), dtype=data.dtype)\n",
        "    for i in range(len_h):\n",
        "        X[i * n: i * n + n, :] = data[:, max_h - h[i]:m - h[i]]\n",
        "    return X, Y\n",
        "\n",
        "\n",
        "def remove_weekends(data, start=0, bs=36):\n",
        "    _, m = data.shape\n",
        "    n_day = int(m / bs)\n",
        "    weekday = np.concatenate([np.arange(start, 7) % 7, np.arange(n_day) % 7])[:n_day]\n",
        "    weekday = np.repeat(weekday, bs)\n",
        "    return data[:, weekday < 5]\n",
        "\n",
        "\n",
        "def get_flow1(od, s, dir='o'):\n",
        "    \"\"\"Get the flow of station `s`\"\"\"\n",
        "    n = od.shape[0]\n",
        "    if dir == 'o':\n",
        "        idx = np.arange(s, n, 159)\n",
        "    elif dir == 'd':\n",
        "        idx = np.arange((s * 159), (s * 159 + 159))\n",
        "    return np.sum(od[idx, :], axis=0)\n",
        "\n",
        "\n",
        "def od2flow(od, s_list=None, dir='o'):\n",
        "    if s_list is None:\n",
        "        s_list = range(159)\n",
        "\n",
        "    n_s = len(s_list)\n",
        "    flow = np.zeros((n_s, od.shape[1]), dtype=np.float32)\n",
        "    for i, s in enumerate(s_list):\n",
        "        flow[i, :] = get_flow1(od, s, dir)\n",
        "    return flow\n",
        "\n",
        "\n",
        "def RMSE(f0, f1, axis=None):\n",
        "    return np.sqrt(np.mean((f0 - f1) ** 2, axis))\n",
        "  \n",
        "\n",
        "def start_end_idx(start, end, weekend=False, night=False):\n",
        "    date = pd.period_range('2017-07-01', '2017-09-30 23:30', freq='30T')\n",
        "    date = date.to_timestamp()\n",
        "    if not night:\n",
        "        date = date[date.hour >= 6]\n",
        "    if not weekend:\n",
        "        date = date[date.weekday < 5]\n",
        "    idx = pd.DataFrame(data=np.arange(date.shape[0]), index=date)\n",
        "    return idx.loc[start:end, :].values.ravel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ts5me2dHS9JS"
      },
      "source": [
        "data0 = loadmat('drive//MyDrive//data//OD_3m.mat')\n",
        "data0 = data0['OD']\n",
        "data0 = remove_weekends(data0, start=5)\n",
        "\n",
        "# The mean in the training set\n",
        "data = data0.astype(np.float64)\n",
        "data_mean = data[:, 0:20*36].reshape([159*159, 36, -1], order='F')\n",
        "data_mean = data_mean.mean(axis=2)\n",
        "for i in range(65):\n",
        "    data[:,i*36:(i+1)*36] = data[:,i*36:(i+1)*36] - data_mean\n",
        "\n",
        "train_idx = start_end_idx('2017-07-03', '2017-07-28', weekend=False, night=False)\n",
        "validate_idx = start_end_idx('2017-07-31', '2017-08-11', weekend=False, night=False)\n",
        "test_idx = start_end_idx('2017-08-14', '2017-08-25', weekend=False, night=False)\n",
        "flow = od2flow(data)\n",
        "\n",
        "h = 10\n",
        "train_data = data[:, train_idx].reshape([159, 159, -1], order='F').transpose([2,0,1])\n",
        "validate_data = data[:, validate_idx[0]-h:test_idx[0]].reshape([159, 159, -1], order='F').transpose([2,0,1])\n",
        "test_data = data[:, test_idx[0]-h:test_idx[-1]+1].reshape([159, 159, -1], order='F').transpose([2,0,1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aG1kQAFVZrVq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17867826-ab89-4cc4-aba2-357e11c07e85"
      },
      "source": [
        "class Data(keras.utils.Sequence):\n",
        "    def __init__(self, data, h=10, batch_size=32):\n",
        "        self.batch_size = batch_size\n",
        "        self.h = h\n",
        "        self.data = data  # (time, O, D)\n",
        "        self.data_length = data.shape[0]\n",
        "        self.length = int(np.ceil((data.shape[0] - h) / batch_size))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Only allows positive idx\n",
        "        if idx < 0:\n",
        "            raise ValueError('idx must be positive')\n",
        "        x_start = self.batch_size * idx\n",
        "        x_end = np.min([self.data_length - self.h, self.batch_size * (idx + 1)])\n",
        "\n",
        "        y_start = self.h + self.batch_size * idx\n",
        "        y_end = np.min([self.data_length, self.h + self.batch_size * (idx + 1)])\n",
        "\n",
        "        # X: (samples, time, rows, cols, channels)\n",
        "        batch_x = np.zeros([x_end-x_start, self.h, 159, 159, 1], dtype=self.data.dtype)\n",
        "        for i, s in enumerate(range(x_start, x_end)):\n",
        "            batch_x[i, :, :, :, :] = self.data[s:s+self.h, :, :, np.newaxis]\n",
        "\n",
        "        # Y: (samples, new_rows, new_cols, filters)\n",
        "        batch_y = self.data[y_start:y_end, :, :, np.newaxis]\n",
        "        return (batch_x, batch_y)\n",
        "\n",
        "train_Data = Data(train_data, h=10, batch_size=8)\n",
        "validate_Data = Data(validate_data, h=10, batch_size=8)\n",
        "\n",
        "\n",
        "seq = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(\n",
        "            shape=(None, 159, 159, 1)\n",
        "        ),  # Variable-length sequence of 159x159x1 frames\n",
        "        layers.ConvLSTM2D(\n",
        "            filters=8, kernel_size=(3, 3), padding=\"same\", return_sequences=True,\n",
        "            data_format='channels_last'\n",
        "        ),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ConvLSTM2D(\n",
        "            filters=8, kernel_size=(3, 3), padding=\"same\", return_sequences=True,\n",
        "            data_format='channels_last'\n",
        "        ),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ConvLSTM2D(\n",
        "            filters=1, kernel_size=(3, 3), padding=\"same\", return_sequences=False,\n",
        "            data_format='channels_last'\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "seq.compile(loss=\"mean_squared_error\", optimizer=\"RMSprop\")\n",
        "seq.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv_lst_m2d (ConvLSTM2D)    (None, None, 159, 159, 8) 2624      \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, None, 159, 159, 8) 32        \n",
            "_________________________________________________________________\n",
            "conv_lst_m2d_1 (ConvLSTM2D)  (None, None, 159, 159, 8) 4640      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, None, 159, 159, 8) 32        \n",
            "_________________________________________________________________\n",
            "conv_lst_m2d_2 (ConvLSTM2D)  (None, 159, 159, 1)       328       \n",
            "=================================================================\n",
            "Total params: 7,656\n",
            "Trainable params: 7,624\n",
            "Non-trainable params: 32\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RnJ5Jq5lf2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c4bc266-0299-4034-8fbf-9af021e44a63"
      },
      "source": [
        "checkpoint_path = 'drive//MyDrive//data//train_sub_mean10__8(3)_8(3)_1(3).ckpt'\n",
        "\n",
        "call_back = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', min_delta=0, patience=20, verbose=0, mode='auto',\n",
        "    baseline=None, restore_best_weights=True\n",
        ")\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1,\n",
        "                                                 )\n",
        "\n",
        "tf.random.set_seed(0)\n",
        "seq.fit(\n",
        "    x=train_Data,\n",
        "    epochs=200,\n",
        "    steps_per_epoch=len(train_Data),\n",
        "    verbose=2,\n",
        "    shuffle=True,\n",
        "    validation_data=validate_Data,\n",
        "    validation_steps=len(validate_Data),\n",
        "    callbacks=[call_back, cp_callback],\n",
        ")\n",
        "seq.save_weights(checkpoint_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "\n",
            "Epoch 00001: saving model to drive//MyDrive//data/train_sub_mean10__8(3)_8(3)_1(3).ckpt\n",
            "89/89 - 43s - loss: 8.7876 - val_loss: 9.7130\n",
            "Epoch 2/200\n",
            "\n",
            "Epoch 00002: saving model to drive//MyDrive//data/train_sub_mean10__8(3)_8(3)_1(3).ckpt\n",
            "89/89 - 44s - loss: 8.7880 - val_loss: 9.7078\n",
            "Epoch 3/200\n",
            "\n",
            "Epoch 00003: saving model to drive//MyDrive//data/train_sub_mean10__8(3)_8(3)_1(3).ckpt\n",
            "89/89 - 44s - loss: 8.7880 - val_loss: 9.7089\n",
            "Epoch 4/200\n",
            "\n",
            "Epoch 00004: saving model to drive//MyDrive//data/train_sub_mean10__8(3)_8(3)_1(3).ckpt\n",
            "89/89 - 45s - loss: 8.7870 - val_loss: 9.7120\n",
            "Epoch 5/200\n",
            "\n",
            "Epoch 00005: saving model to drive//MyDrive//data/train_sub_mean10__8(3)_8(3)_1(3).ckpt\n",
            "89/89 - 46s - loss: 8.7866 - val_loss: 9.7028\n",
            "Epoch 6/200\n",
            "\n",
            "Epoch 00006: saving model to drive//MyDrive//data/train_sub_mean10__8(3)_8(3)_1(3).ckpt\n",
            "89/89 - 46s - loss: 8.7862 - val_loss: 9.7081\n",
            "Epoch 7/200\n",
            "\n",
            "Epoch 00007: saving model to drive//MyDrive//data/train_sub_mean10__8(3)_8(3)_1(3).ckpt\n",
            "89/89 - 46s - loss: 8.7861 - val_loss: 9.7080\n",
            "Epoch 8/200\n",
            "\n",
            "Epoch 00008: saving model to drive//MyDrive//data/train_sub_mean10__8(3)_8(3)_1(3).ckpt\n",
            "89/89 - 46s - loss: 8.7851 - val_loss: 9.7072\n",
            "Epoch 9/200\n",
            "\n",
            "Epoch 00009: saving model to drive//MyDrive//data/train_sub_mean10__8(3)_8(3)_1(3).ckpt\n",
            "89/89 - 46s - loss: 8.7854 - val_loss: 9.7051\n",
            "Epoch 10/200\n",
            "\n",
            "Epoch 00010: saving model to drive//MyDrive//data/train_sub_mean10__8(3)_8(3)_1(3).ckpt\n",
            "89/89 - 46s - loss: 8.7840 - val_loss: 9.7047\n",
            "Epoch 11/200\n",
            "\n",
            "Epoch 00011: saving model to drive//MyDrive//data/train_sub_mean10__8(3)_8(3)_1(3).ckpt\n",
            "89/89 - 46s - loss: 8.7838 - val_loss: 9.7082\n",
            "Epoch 12/200\n",
            "\n",
            "Epoch 00012: saving model to drive//MyDrive//data/train_sub_mean10__8(3)_8(3)_1(3).ckpt\n",
            "89/89 - 46s - loss: 8.7836 - val_loss: 9.7035\n",
            "Epoch 13/200\n",
            "\n",
            "Epoch 00013: saving model to drive//MyDrive//data/train_sub_mean10__8(3)_8(3)_1(3).ckpt\n",
            "89/89 - 46s - loss: 8.7829 - val_loss: 9.7071\n",
            "Epoch 14/200\n",
            "\n",
            "Epoch 00014: saving model to drive//MyDrive//data/train_sub_mean10__8(3)_8(3)_1(3).ckpt\n",
            "89/89 - 46s - loss: 8.7825 - val_loss: 9.7094\n",
            "Epoch 15/200\n",
            "\n",
            "Epoch 00015: saving model to drive//MyDrive//data/train_sub_mean10__8(3)_8(3)_1(3).ckpt\n",
            "89/89 - 46s - loss: 8.7825 - val_loss: 9.7200\n",
            "Epoch 16/200\n",
            "\n",
            "Epoch 00016: saving model to drive//MyDrive//data/train_sub_mean10__8(3)_8(3)_1(3).ckpt\n",
            "89/89 - 46s - loss: 8.7822 - val_loss: 9.7094\n",
            "Epoch 17/200\n",
            "\n",
            "Epoch 00017: saving model to drive//MyDrive//data/train_sub_mean10__8(3)_8(3)_1(3).ckpt\n",
            "89/89 - 46s - loss: 8.7810 - val_loss: 9.7087\n",
            "Epoch 18/200\n",
            "\n",
            "Epoch 00018: saving model to drive//MyDrive//data/train_sub_mean10__8(3)_8(3)_1(3).ckpt\n",
            "89/89 - 46s - loss: 8.7818 - val_loss: 9.7082\n",
            "Epoch 19/200\n",
            "\n",
            "Epoch 00019: saving model to drive//MyDrive//data/train_sub_mean10__8(3)_8(3)_1(3).ckpt\n",
            "89/89 - 46s - loss: 8.7811 - val_loss: 9.7133\n",
            "Epoch 20/200\n",
            "\n",
            "Epoch 00020: saving model to drive//MyDrive//data/train_sub_mean10__8(3)_8(3)_1(3).ckpt\n",
            "89/89 - 46s - loss: 8.7808 - val_loss: 9.7072\n",
            "Epoch 21/200\n",
            "\n",
            "Epoch 00021: saving model to drive//MyDrive//data/train_sub_mean10__8(3)_8(3)_1(3).ckpt\n",
            "89/89 - 46s - loss: 8.7805 - val_loss: 9.7041\n",
            "Epoch 22/200\n",
            "\n",
            "Epoch 00022: saving model to drive//MyDrive//data/train_sub_mean10__8(3)_8(3)_1(3).ckpt\n",
            "89/89 - 46s - loss: 8.7803 - val_loss: 9.7079\n",
            "Epoch 23/200\n",
            "\n",
            "Epoch 00023: saving model to drive//MyDrive//data/train_sub_mean10__8(3)_8(3)_1(3).ckpt\n",
            "89/89 - 46s - loss: 8.7809 - val_loss: 9.7139\n",
            "Epoch 24/200\n",
            "\n",
            "Epoch 00024: saving model to drive//MyDrive//data/train_sub_mean10__8(3)_8(3)_1(3).ckpt\n",
            "89/89 - 46s - loss: 8.7794 - val_loss: 9.7087\n",
            "Epoch 25/200\n",
            "\n",
            "Epoch 00025: saving model to drive//MyDrive//data/train_sub_mean10__8(3)_8(3)_1(3).ckpt\n",
            "89/89 - 46s - loss: 8.7791 - val_loss: 9.7124\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTGOFK4Ej3qp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "066e3e29-4bc3-48f8-8fd3-b2aaf18aa35c"
      },
      "source": [
        "seq.load_weights('/content/drive/MyDrive/data/train_sub_mean10__8(3)_8(3)_1(3).ckpt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f537e1fca58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DOLUgOMbZwX",
        "outputId": "502430e2-d9ad-47e9-d5ca-8665fd7feddc"
      },
      "source": [
        "h=10\n",
        "t_data = train_data\n",
        "n = t_data.shape[0]\n",
        "result = np.zeros((n-h, 159, 159))\n",
        "for i in range(n-h):\n",
        "     result[i, :, :] = seq.predict(t_data[np.newaxis, i:i+h, :, :, np.newaxis]).squeeze()\n",
        "\n",
        "RMSE(t_data[h:,:,:], result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.965144170344829"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "019vxzSgbDb4",
        "outputId": "a8a942fd-a281-46cd-ee61-40d63325f9f2"
      },
      "source": [
        "h=10\n",
        "t_data = validate_data\n",
        "n = t_data.shape[0]\n",
        "result = np.zeros((n-h, 159, 159))\n",
        "for i in range(n-h):\n",
        "     result[i, :, :] = seq.predict(t_data[np.newaxis, i:i+h, :, :, np.newaxis]).squeeze()\n",
        "\n",
        "RMSE(t_data[h:,:,:], result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.1149310041459413"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDR0xIcgZqnb",
        "outputId": "85f2a1be-66c3-4330-92d5-5d7b907cdf2d"
      },
      "source": [
        "h=10\n",
        "t_data = test_data\n",
        "n = t_data.shape[0]\n",
        "result = np.zeros((n-h, 159, 159))\n",
        "for i in range(n-h):\n",
        "     result[i, :, :] = seq.predict(t_data[np.newaxis, i:i+h, :, :, np.newaxis]).squeeze()\n",
        "\n",
        "RMSE(t_data[h:,:,:], result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.3807796261219565"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8WjJlC-lGRN"
      },
      "source": [
        "result = result.transpose([1,2,0]).reshape([159*159, -1], order='F')\n",
        "for i in range(result.shape[1]):\n",
        "  result[:, i] += data_mean[:, i%36]\n",
        "np.savez_compressed('/content/drive/MyDrive/data/OD_ConvLSTM_sub_mean_1.npz', data=result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRX9Ll1CeXKD"
      },
      "source": [
        "# Multi-step forecast"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_MnGOBjdwd3",
        "outputId": "2c10d3dd-bd08-4df7-a741-ecaed3f83a0c"
      },
      "source": [
        "# 3-step forecast\r\n",
        "h = 10\r\n",
        "\r\n",
        "t_data = data[:, test_idx[0]-h-2:test_idx[-1]+1].reshape([159, 159, -1], order='F').transpose([2,0,1])\r\n",
        "n = t_data.shape[0]\r\n",
        "results = {step+1: np.zeros((n-h, 159, 159)) for step in range(3)}\r\n",
        "\r\n",
        "for i in range(n-h):\r\n",
        "    X1 = t_data[np.newaxis, i:i+h, :, :, np.newaxis]\r\n",
        "    results[1][i, :, :] = seq.predict(X1).squeeze()\r\n",
        "    X2 = np.concatenate([X1[:, 1:, :, :, :], results[1][np.newaxis, [i], :, :, np.newaxis]], axis=1)\r\n",
        "    results[2][i, :, :] = seq.predict(X2).squeeze()\r\n",
        "    X3 = np.concatenate([X2[:, 1:, :, :, :], results[2][np.newaxis, [i], :, :, np.newaxis]], axis=1)\r\n",
        "    results[3][i, :, :] = seq.predict(X3).squeeze()\r\n",
        "\r\n",
        "print(RMSE(t_data[h+2:,:,:], results[1][2:, :,:]))\r\n",
        "print(RMSE(t_data[h+2:,:,:], results[2][1:-1, :,:]))\r\n",
        "print(RMSE(t_data[h+2:,:,:], results[3][0:-2, :,:]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.3807796261219565\n",
            "3.3932118112296785\n",
            "3.4031268503844174\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQRr9cIavBgi"
      },
      "source": [
        "result1 = results[1][2:, :,:].transpose([1,2,0]).reshape([159*159, -1], order='F')\r\n",
        "for i in range(result1.shape[1]):\r\n",
        "  result1[:, i] += data_mean[:, i%36]\r\n",
        "\r\n",
        "result2 = results[2][1:-1, :,:].transpose([1,2,0]).reshape([159*159, -1], order='F')\r\n",
        "for i in range(result2.shape[1]):\r\n",
        "  result2[:, i] += data_mean[:, i%36]\r\n",
        "\r\n",
        "result3 = results[3][0:-2, :,:].transpose([1,2,0]).reshape([159*159, -1], order='F')\r\n",
        "for i in range(result3.shape[1]):\r\n",
        "  result3[:, i] += data_mean[:, i%36]\r\n",
        "\r\n",
        "np.savez_compressed('/content/drive/MyDrive/data/OD_ConvLSTM_step1.npz', data=result1)\r\n",
        "np.savez_compressed('/content/drive/MyDrive/data/OD_ConvLSTM_step2.npz', data=result2)\r\n",
        "np.savez_compressed('/content/drive/MyDrive/data/OD_ConvLSTM_step3.npz', data=result3)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}